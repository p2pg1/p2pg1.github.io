### 翻译结果：

===== 第1页 =====

# 神经网络中的知识蒸馏

杰弗里·辛顿*  
谷歌公司  
山景城  
geoffhinton@google.com  

奥里奥尔·维尼亚尔斯†  
谷歌公司  
山景城  
vinyals@google.com  

杰夫·迪恩  
谷歌公司  
山景城  
jeff@google.com  

*同时隶属于多伦多大学和加拿大高等研究院。†同等贡献。

###### 摘要

提升几乎所有机器学习算法性能的一种非常简单的方法是：在同一数据上训练多个不同的模型，然后对它们的预测结果进行平均[3]。然而，使用整个模型集合进行预测非常繁琐，并且计算成本可能过高，难以部署到大量用户中，尤其是当单个模型是大型神经网络时。Caruana及其合作者[1]已证明，可以将集成模型中的知识压缩到一个更易于部署的单一模型中。我们采用了一种不同的压缩技术，进一步发展了这一方法。我们在MNIST上取得了一些令人惊讶的结果，并证明通过将集成模型中的知识蒸馏到单一模型中，可以显著改进一个广泛使用的商业系统的声学模型。我们还引入了一种新型集成模型，它由一个或多个完整模型和许多专注于区分完整模型容易混淆的细粒度类别的“专家模型”组成。与专家混合模型不同，这些专家模型可以快速并行训练。

## 1 引言

许多昆虫有一种幼虫形态，专注于从环境中获取能量和营养，而完全不同的成虫形态则专注于满足迁移和繁殖的需求。在大规模机器学习中，尽管训练阶段和部署阶段的需求差异很大，我们通常使用非常相似的模型：对于语音和物体识别等任务，训练阶段需要从庞大且高度冗余的数据集中提取结构，但不需要实时运行，且可以使用大量计算资源。然而，部署到大量用户时，对延迟和计算资源的要求更为严格。昆虫的类比提示我们，如果能够更容易地从数据中提取结构，我们应愿意训练非常复杂的模型。这种复杂模型可以是由多个独立训练的模型组成的集成模型，也可以是通过强正则化（如dropout[9]）训练的单一大型模型。一旦复杂模型训练完成，我们可以使用一种称为“蒸馏”的不同训练方式，将其知识转移到更适合部署的小型模型中。Rich Caruana及其合作者[1]已经开创了这一策略的雏形。在他们的重要论文中，他们令人信服地证明了通过大型模型集成获得的知识可以转移到单一小型模型中。

阻碍这一方法进一步研究的一个概念障碍是：我们倾向于将训练模型中的知识与学习到的参数值等同起来，这使得我们难以理解如何在改变模型形式的同时保留相同的知识。一种更抽象的知识观点是将其视为从输入向量到输出向量的学习映射。对于能够区分大量类别的复杂模型，正常的训练目标是最大化正确答案的平均对数概率，但学习的副作用是训练模型会为所有错误答案分配概率，即使这些概率非常小，某些错误答案的概率仍远高于其他错误答案。错误答案的相对概率揭示了复杂模型的泛化倾向。例如，一张宝马汽车的照片被误认为垃圾车的概率可能非常小，但仍比被误认为胡萝卜的概率高出许多倍。

通常认为，训练使用的目标函数应尽可能反映用户的真实目标。尽管如此，模型通常被训练为优化训练数据上的性能，而真实目标是在新数据上良好泛化。显然，训练模型以更好地泛化是更优的选择，但这需要关于正确泛化方式的信息，而这些信息通常不可得。然而，当我们将大型模型的知识蒸馏到小型模型时，可以训练小型模型以与大型模型相同的方式泛化。如果复杂模型因其为多个不同模型的平均而泛化良好，那么以相同方式训练的小型模型在测试数据上的表现通常会优于直接在相同训练集上以常规方式训练的小型模型。

将复杂模型的泛化能力转移到小型模型的一种直接方法是使用复杂模型生成的类别概率作为训练小型模型的“软目标”。在此转移阶段，可以使用相同的训练集或单独的“转移集”。当复杂模型是多个简单模型的集成时，可以使用其预测分布的算术或几何平均作为软目标。当软目标的熵较高时，每个训练样本提供的信息比硬目标更多，且梯度在不同训练样本间的方差更小，因此小型模型通常可以在比原始复杂模型少得多的数据上训练，并使用更高的学习率。

对于像MNIST这样的任务，复杂模型几乎总是以极高的置信度给出正确答案，学习到的函数的大部分信息隐藏在软目标中非常小的概率比值中。例如，某个数字“2”可能被赋予“3”的概率为10^-6，而被赋予“7”的概率为10^-9，而另一个“2”的概率分布可能相反。这些信息定义了数据间丰富的相似性结构（即哪些“2”看起来像“3”，哪些像“7”），但由于概率接近于零，它们在转移阶段的交叉熵成本函数中影响甚微。Caruana及其合作者通过使用逻辑值（softmax的输入）而非softmax生成的概率作为小型模型的学习目标，并最小化复杂模型与小型模型逻辑值之间的平方差，解决了这一问题。我们更通用的解决方案称为“蒸馏”，即通过提高最终softmax的温度，使复杂模型生成一组适合的软目标。然后在训练小型模型匹配这些软目标时使用相同的高温。我们稍后将证明，匹配复杂模型的逻辑值实际上是蒸馏的一种特例。

用于训练小型模型的转移集可以完全由未标记数据组成[1]，也可以使用原始训练集。我们发现使用原始训练集效果良好，尤其是在目标函数中添加一个小项以鼓励小型模型预测真实目标的同时匹配复杂模型提供的软目标时。通常，小型模型无法完全匹配软目标，而在正确方向上的偏差反而有益。

## 2 蒸馏

神经网络通常通过“softmax”输出层生成类别概率，该层将每个类别的逻辑值\(z_i\)转换为概率\(q_i\)，通过与其他逻辑值比较实现：

\[q_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)} \quad (1)\]

===== 第2页 =====

其中\(T\)是温度，通常设为1。使用更高的\(T\)值会生成更平滑的类别概率分布。

在最简单的蒸馏形式中，知识通过以下方式转移到蒸馏模型：在转移集上训练蒸馏模型，并使用复杂模型在其softmax中设置高温生成的软目标分布。训练蒸馏模型时使用相同的高温，但训练完成后将温度恢复为1。

当转移集中的部分或全部样本已知正确标签时，可以通过同时训练蒸馏模型生成正确标签来显著改进此方法。一种方法是使用正确标签修改软目标，但我们发现更好的方式是简单使用两个不同目标函数的加权平均。第一个目标函数是与软目标的交叉熵，计算时在蒸馏模型的softmax中使用与生成软目标相同的高温。第二个目标函数是与正确标签的交叉熵，计算时在蒸馏模型的softmax中使用完全相同的逻辑值，但温度为1。我们发现，通常通过大幅降低第二个目标函数的权重可以获得最佳结果。由于软目标生成的梯度幅值与\(1/T^2\)成正比，因此在使用硬目标和软目标时将其乘以\(T^2\)非常重要。这样可以确保在调整蒸馏温度时，硬目标和软目标的相对贡献大致保持不变。

### 匹配逻辑值是蒸馏的特例

转移集中的每个样本对蒸馏模型的每个逻辑值\(z_i\)贡献一个交叉熵梯度\(\partial C / \partial z_i\)。如果复杂模型的逻辑值为\(v_i\)，生成的软目标概率为\(p_i\)，且转移训练的温度为\(T\)，则该梯度为：

\[\frac{\partial C}{\partial z_i} = \frac{1}{T} (q_i - p_i) = \frac{1}{T} \left( \frac{e^{z_i / T}}{\sum_j e^{z_j / T}} - \frac{e^{v_i / T}}{\sum_j e^{v_j / T}} \right) \quad (2)\]

如果温度远高于逻辑值的幅值，可以近似为：

\[\frac{\partial C}{\partial z_i} \approx \frac{1}{T} \left( \frac{1 + z_i / T}{N + \sum_j z_j / T} - \frac{1 + v_i / T}{N + \sum_j v_j / T} \right) \quad (3)\]

假设每个转移样本的逻辑值已分别零均值化，即\(\sum_j z_j = \sum_j v_j = 0\)，则式(3)简化为：

\[\frac{\partial C}{\partial z_i} \approx \frac{1}{NT^2} (z_i - v_i) \quad (4)\]

因此在高温极限下，蒸馏等价于最小化\(\frac{1}{2}(z_i - v_i)^2\)，前提是每个转移样本的逻辑值已零均值化。在较低温度下，蒸馏对远低于平均值的逻辑值的关注度大幅降低。这可能是有利的，因为这些逻辑值在训练复杂模型时几乎不受成本函数的约束，可能包含大量噪声。另一方面，这些极负的逻辑值可能传递复杂模型学习到的重要知识。哪种效应占主导是一个实证问题。我们表明，当蒸馏模型过小无法捕获复杂模型中的所有知识时，中等温度效果最佳，这强烈表明忽略大幅负逻辑值可能有益。

## 3 MNIST上的初步实验

为验证蒸馏效果，我们训练了一个单一大型神经网络，包含两个1200个修正线性单元的隐藏层，使用全部60000个训练样本。网络通过dropout和权重约束[5]进行了强正则化。Dropout可视为训练共享权重的指数级规模模型集成的一种方式。此外，输入图像在任意方向上最多偏移两个像素。该网络在测试集上实现了67个错误，而一个较小的网络（两个800个修正线性单元的隐藏层，无正则化）实现了146个错误。但如果小网络仅通过匹配大型网络在温度为20时生成的软目标进行正则化，则实现了74个测试错误。这表明软目标可以向蒸馏模型传递大量知识，包括从平移训练数据中学到的泛化知识，即使转移集中不包含任何平移样本。

当蒸馏网络的两个隐藏层各有300或更多单元时，所有高于8的温度均给出相似结果。但当每层单元数大幅减少至30时，温度在2.5至4范围内的效果显著优于更高或更低的温度。

随后，我们尝试从转移集中完全省略数字“3”的样本。因此，从蒸馏模型的角度看，“3”是一个从未见过的虚构数字。尽管如此，蒸馏模型仅产生206个测试错误，其中133个出现在测试集的1010个“3”上。大多数错误是由于“3”类别的学习偏置过低。如果将该偏置增加3.5（以优化测试集整体性能），蒸馏模型的错误降至109个，其中仅14个为“3”。因此，在正确偏置下，蒸馏模型对测试集中“3”的识别准确率达到98.6%，尽管训练期间从未见过“3”。如果转移集仅包含训练集中的“7”和“8”，蒸馏模型的测试错误率为47.3%，但将“7”和“8”的偏置降低7.6以优化测试性能后，错误率降至13.2%。

## 4 语音识别实验

本节研究了集成深度神经网络（DNN）声学模型在自动语音识别（ASR）中的效果。我们证明，本文提出的蒸馏策略成功将模型集成的知识蒸馏到单一模型中，其性能显著优于直接从相同训练数据学习的同规模模型。

当前最先进的ASR系统使用DNN将（短时）声学特征的时序上下文映射到隐马尔可夫模型（HMM）离散状态的概率分布[4]。具体而言，DNN在每一时间步生成三音素状态簇的概率分布，解码器随后在HMM状态中找到一条路径，该路径是高概率状态与语言模型下可能转录之间的最佳折衷。

尽管可以通过对所有可能路径进行边缘化来训练DNN（考虑解码器和语言模型），但通常训练DNN进行逐帧分类，通过（局部）最小化网络预测与真实状态序列强制对齐标签之间的交叉熵：

\[\boldsymbol{\theta} = \arg \max_{\boldsymbol{\theta}'} P(h_t | \mathbf{s}_t; \boldsymbol{\theta}')\]

其中\(\boldsymbol{\theta}\)是声学模型\(P\)的参数，该模型将时间\(t\)的声学观测\(\mathbf{s}_t\)映射到HMM状态\(h_t\)的概率\(P(h_t | \mathbf{s}_t; \boldsymbol{\theta}')\)，\(h_t\)由正确词序列的强制对齐确定。模型通过分布式随机梯度下降法训练。

我们使用的架构包含8个隐藏层，每层2560个修正线性单元，以及一个14000个标签（HMM目标\(h_t\)）的最终softmax层。输入为26帧40维梅尔滤波器组系数，每帧间隔10毫秒，预测第21帧的HMM状态。总参数量约8500万。这是Android语音搜索声学模型的略微过时版本，应视为非常强的基线。训练DNN声学模型使用了约2000小时的英语语音数据，生成约7亿训练样本。该系统在开发集上的帧准确率为58.9%，词错误率（WER）为10.9%。

===== 第3页 =====

### 结果

我们训练了10个独立模型预测\(P(h_t | \mathbf{s}_t; \boldsymbol{\theta})\)，架构和训练流程与基线完全相同。模型通过不同的随机参数初始化，我们发现这足以在训练模型中产生足够的多样性，使得集成模型的平均预测显著优于单个模型。我们尝试通过改变每个模型的数据集来增加多样性，但发现效果不显著，因此选择了更简单的方法。蒸馏时尝试了温度\([1, 2, 5, 10]\)，并在硬目标的交叉熵上使用0.5的相对权重，表1中加粗的值为最佳结果。

表1显示，蒸馏方法确实能够从训练集中提取比直接使用硬标签训练单一模型更多有用的信息。使用10个模型集成带来的帧分类准确率提升中，超过80%转移到了蒸馏模型，这与我们在MNIST上的初步实验结果相似。由于目标函数不匹配，集成在最终目标WER（基于23K词测试集）上的提升较小，但集成带来的WER改进同样转移到了蒸馏模型。

我们最近注意到相关工作[8]通过匹配已训练大型模型的类别概率来学习小型声学模型。然而，他们在温度1下使用大型未标记数据集进行蒸馏，其最佳蒸馏模型仅将小型模型的错误率降低了大型和小型模型在硬标签训练下错误率差距的28%。

## 5 在超大数据集上训练专家模型集成

训练模型集成是利用并行计算的简单方法，而集成在测试时计算量过大的问题可以通过蒸馏解决。然而，集成还有另一个重要缺点：如果单个模型是大型神经网络且数据集非常大，即使并行化容易，训练阶段的计算量也过高。

本节以这样一个数据集为例，展示通过学习专注于不同易混淆类别子集的专家模型如何减少学习集成所需的总计算量。专注于细粒度区分的专家模型的主要问题是容易过拟合，我们描述了如何通过使用软目标防止这种过拟合。

### JFT数据集

JFT是谷歌内部数据集，包含1亿张标注图像和15000个标签。在进行此项工作时，谷歌的JFT基线模型是一个深度卷积神经网络[7]，已在大规模核心集群上通过异步随机梯度下降训练约六个月。训练采用两种并行方式[2]：首先，多个神经网络副本在不同核心集上运行，处理训练集的不同小批量。每个副本计算当前小批量的平均梯度并发送到分片参数服务器，后者返回参数新值。这些新值反映了参数服务器自上次发送参数以来接收的所有梯度。其次，每个副本通过将不同神经元子集分配到不同核心上实现并行。集成训练是第三种并行方式，可以包装前两种方式，但需要更多核心。等待数年训练模型集成不可行，因此我们需要更快改进基线模型的方法。

### 专家模型

当类别数量非常大时，复杂模型可以是一个包含一个通用模型（在所有数据上训练）和多个“专家模型”的集成，每个专家模型专注于高度易混淆的类别子集（如不同类型的蘑菇）。通过将所有不关心的类别合并为一个“垃圾桶”类别，可以大幅减小此类专家的softmax规模。

为减少过拟合并共享低层特征检测器的学习，每个专家模型以通用模型的权重初始化。然后通过训练专家模型略微调整这些权重，其中一半样本来自其特殊子集，另一半从剩余训练集中随机采样。训练后，可以通过按专家类别过采样比例的对数增加垃圾桶类别的逻辑值来校正训练集的偏差。

### 为专家分配类别

为确定专家模型的类别分组，我们专注于全网络经常混淆的类别。尽管可以计算混淆矩阵并用于聚类，我们选择了一种更简单的方法，无需真实标签构建聚类。

具体而言，我们对通用模型预测的协方差矩阵应用聚类算法，使得经常被同时预测的一组类别\(S^m\)成为专家模型\(m\)的目标。我们对协方差矩阵的列应用在线K均值算法，获得了合理的聚类（如表2所示）。尝试的其他聚类算法也产生了类似结果。

### 使用专家模型集成进行推断

在研究蒸馏专家模型的效果之前，我们希望了解包含专家的集成表现如何。除专家模型外，我们始终保留通用模型以处理无专家覆盖的类别并决定使用哪些专家。给定输入图像\(\mathbf{x}\)，我们分两步进行Top-1分类：

步骤1：对每个测试样本，找到通用模型预测的最可能的\(n\)个类别集合\(k\)。实验中\(n=1\)。

步骤2：选取所有特殊子集\(S^m\)与\(k\)有非空交集的专家模型\(m\)，称为活跃专家集\(A_k\)（可能为空）。然后找到最小化以下公式的全类别概率分布\(\mathbf{q}\)：

\[KL(\mathbf{p}^g, \mathbf{q}) + \sum_{m \in A_k} KL(\mathbf{p}^m, \mathbf{q}) \quad (5)\]

其中\(KL\)表示KL散度，\(\mathbf{p}^m\)和\(\mathbf{p}^g\)分别是专家模型或通用全模型的概率分布。\(\mathbf{p}^m\)是专家类别加一个垃圾桶类别的分布，因此计算其与全分布\(\mathbf{q}\)的KL散度时，需将\(\mathbf{q}\)分配给垃圾桶类别的所有类别概率求和。

式(5)无通用闭式解，但当所有模型为每个类别生成单一概率时，解为算术或几何平均（取决于使用\(KL(\mathbf{p}, \mathbf{q})\)还是\(KL(\mathbf{q}, \mathbf{p})\)）。我们参数化\(\mathbf{q} = \text{softmax}(\mathbf{z})\)（\(T=1\)）并使用梯度下降优化式(5)中的逻辑值\(\mathbf{z}\)。注意此优化需对每张图像进行。

### 结果

从训练好的基线全网络开始，专家模型训练极快（JFT上仅需数天而非数周）。所有专家模型完全独立训练。表3显示了基线系统及与专家模型结合的基线系统的绝对测试准确率。使用61个专家模型时，整体测试准确率相对提升4.4%。我们还报告了条件测试准确率，即仅考虑属于专家类别的样本并将预测限制在该子集时的准确率。

在JFT专家实验中，我们训练了61个专家模型，每个模型包含300个类别（加一个垃圾桶类别）。由于专家模型的类别集不互斥，通常多个专家覆盖同一图像类别。表4显示了测试集样本数量、使用专家后Top1正确的变化以及按覆盖类别专家数分组的JFT数据集Top1准确率相对提升。我们欣喜地发现，覆盖同一类别的专家越多，准确率提升越大，而训练独立专家模型非常易于并行化。

## 6 软目标作为正则化器

我们关于使用软目标而非硬目标的主要观点之一是：软目标可以携带大量无法通过单一硬目标编码的有用信息。本节通过使用极少数据拟合前述8500万参数的基线语音模型，证明这是一个非常大的效应。表5显示，仅使用3%数据（约2000万样本）时，硬目标训练的基线模型严重过拟合（我们进行了早停，因为准确率在达到44.5%后急剧下降），而相同模型使用软目标训练几乎恢复了完整训练集中的所有信息（仅差2%）。更值得注意的是，我们无需早停：软目标系统直接“收敛”到57%。这表明软目标是将完整数据训练的模型发现的规律传递给另一模型的极有效方式。

===== 第4页 =====

### 使用软目标防止专家过拟合

JFT实验中使用的专家模型将所有非专家类别合并为一个垃圾桶类别。如果允许专家的softmax覆盖所有类别，可能有比早停更好的防止过拟合方法。专家在高度富集其特殊类别的数据上训练，这意味着其有效训练集规模小得多，极易在特殊类别上过拟合。通过大幅减小专家规模无法解决此问题，因为这会丢失建模所有非专家类别带来的有益迁移效应。

使用3%语音数据的实验强烈表明，如果用通用模型权重初始化专家，并通过软目标（通用模型提供）对非特殊类别进行额外训练，可以保留几乎所有非特殊类别的知识。我们正在探索这一方法。

## 7 与专家混合的关系

在数据子集上训练专家的方法与专家混合[6]有相似之处，后者使用门控网络计算将每个样本分配给每个专家的概率。在专家学习处理分配样本的同时，门控网络根据专家对样本的判别性能学习分配概率。使用专家判别性能确定分配比简单聚类输入向量并为每个聚类分配专家更优，但使训练难以并行化：首先，每个专家的加权训练集以依赖所有其他专家的方式不断变化；其次，门控网络需要比较不同专家在同一样本上的性能以调整分配概率。这些困难导致专家混合很少用于可能最有益的场景：包含明显不同子集的超大数据集任务。

并行训练多个专家容易得多。我们首先训练通用模型，然后使用混淆矩阵定义专家的训练子集。一旦子集确定，专家可以完全独立训练。测试时，使用通用模型的预测决定哪些专家相关，仅需运行这些专家。

## 8 讨论

我们已证明，蒸馏在将知识从集成或大型强正则化模型转移到小型蒸馏模型方面效果显著。在MNIST上，即使转移集缺少一个或多个类别的样本，蒸馏效果依然出色。对于Android语音搜索使用的深度声学模型，我们表明通过训练深度神经网络集成获得的几乎所有改进均可蒸馏到同规模的单一神经网络中，极大简化了部署。

对于超大型神经网络，训练完整集成可能不可行，但我们表明，通过训练大量专家网络（每个学习区分高度易混淆的类别簇），可以显著改进单一大型长时间训练网络的性能。我们尚未证明可以将专家的知识蒸馏回单一大型网络。

## 致谢

感谢贾扬清在ImageNet模型训练上的协助，以及Ilya Sutskever和Yoram Singer的有益讨论。

## 参考文献

[1] C. Bucilua, R. Caruana, and A. Niculescu-Mizil. Model compression. In _Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '06, pages 535-541, New York, NY, USA, 2006. ACM.

[2] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In _NIPS_, 2012.

[3] T. G. Dietterich. Ensemble methods in machine learning. In _Multiple classifier systems_, pages 1-15. Springer, 2000.

[4] G. E. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. _Signal Processing Magazine, IEEE_, 29(6):82-97, 2012.

[5] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. _arXiv preprint arXiv:1207.0580_, 2012.

[6] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. _Neural computation_, 3(1):79-87, 1991.

[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In _Advances in Neural Information Processing Systems_, pages 1097-1105, 2012.

[8] J. Li, R. Zhao, J. Huang, and Y. Gong. Learning small-size dnn with output-distribution-based criteria. In _Proceedings Interspeech 2014_, pages 1910-1914, 2014.

[9] N. Srivastava, G.E. Hinton, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. _The Journal of Machine Learning Research_, 15(1):1929-1958, 2014.
